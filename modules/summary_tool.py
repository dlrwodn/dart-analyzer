# modules/summary_tool.py

import streamlit as st
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM

@st.cache_resource
def load_model():
    tokenizer = AutoTokenizer.from_pretrained("digit82/kobart-summarization")
    model = AutoModelForSeq2SeqLM.from_pretrained("digit82/kobart-summarization")
    return tokenizer, model

def run_summary_app():
    st.subheader("ğŸ“„ ë¬¸ì„œ ìš”ì•½ê¸° (KoBART ê¸°ë°˜)")
    st.markdown("í…ìŠ¤íŠ¸ë¥¼ ë¶™ì—¬ë„£ê±°ë‚˜ íŒŒì¼ì„ ì—…ë¡œë“œí•˜ë©´ 3ì¤„ ìš”ì•½ì„ ìƒì„±í•©ë‹ˆë‹¤.")

    uploaded_file = st.file_uploader("ğŸ“‚ í…ìŠ¤íŠ¸ íŒŒì¼ ì—…ë¡œë“œ (.txt)", type="txt")
    manual_text = st.text_area("ë˜ëŠ” ì§ì ‘ ë¬¸ì„œ ì…ë ¥:", height=300)

    if uploaded_file:
        text = uploaded_file.read().decode("utf-8")
    else:
        text = manual_text

    if st.button("ìš”ì•½í•˜ê¸°"):
        if not text.strip():
            st.warning("ë¬¸ì„œë¥¼ ì…ë ¥í•˜ê±°ë‚˜ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”.")
            return

        tokenizer, model = load_model()
        inputs = tokenizer.encode(text, return_tensors="pt", max_length=1024, truncation=True)
        summary_ids = model.generate(inputs, max_length=128, num_beams=4, early_stopping=True)
        summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)

        st.success("âœ… ìš”ì•½ ê²°ê³¼:")
        st.write(summary)